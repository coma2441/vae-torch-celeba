# VAE CelebA - Clean & Organized

A Variational Autoencoder (VAE) implementation for CelebA face generation with a clean, compact structure.

## ğŸ—‚ï¸ Project Structure

```
vae-torch-celeba/
â”œâ”€â”€ vae.py              # VAE model architecture
â”œâ”€â”€ train.py            # Training script  
â”œâ”€â”€ inference.py        # Smart inference with auto model detection
â”œâ”€â”€ utils.py            # Utility functions
â”œâ”€â”€ data/               # CelebA dataset
â”œâ”€â”€ models/             # Trained model files
â”‚   â”œâ”€â”€ vae_quick_trained.pth
â”‚   â”œâ”€â”€ vae_trained_from_scratch.pth
â”‚   â””â”€â”€ vae_model_20.pth
â””â”€â”€ outputs/            # Generated images
    â”œâ”€â”€ generated_random.jpg
    â”œâ”€â”€ latent_interpolation.jpg
    â””â”€â”€ reconstruction_test.jpg
```

## ğŸš€ Quick Start

### 1. Training a New Model
```bash
# Set MPS fallback for compatibility
export PYTORCH_ENABLE_MPS_FALLBACK=1

# Train from scratch (creates models/vae_trained_from_scratch.pth)
python train.py
```

### 2. Generate Images
```bash
# Smart inference - automatically uses best available model
python inference.py
```

## ğŸ“‹ Features

- **ğŸ§  Smart Model Detection**: Automatically finds and uses the best available model
- **ğŸš€ GPU Acceleration**: Uses MPS (Apple Silicon) or CUDA when available
- **ğŸ“¸ Multiple Generation Modes**: 
  - Random face generation
  - Latent space interpolation
  - Image reconstruction
- **ğŸ¯ Clean Architecture**: Organized folder structure with clear separation

## ğŸ¨ Generated Outputs

The inference script creates three types of visualizations:

1. **Random Generation** (`outputs/generated_random.jpg`): 16 randomly generated faces
2. **Latent Interpolation** (`outputs/latent_interpolation.jpg`): Smooth morphing between faces
3. **Reconstruction** (`outputs/reconstruction_test.jpg`): Original vs reconstructed comparison

## ğŸ”§ Configuration

- **Image Size**: 150x150 pixels
- **Latent Dimensions**: 128
- **Architecture**: Convolutional encoder-decoder with batch normalization
- **Training**: Adam optimizer with MSE reconstruction + KL divergence loss

## ğŸ“Š Model Priority

The inference script automatically selects models in this order:
1. `models/vae_quick_trained.pth` (if available)
2. `models/vae_trained_from_scratch.pth` (if available)  
3. `models/vae_model_20.pth` (original pretrained)

## ğŸ› ï¸ Requirements

- PyTorch with MPS/CUDA support
- torchvision
- Pillow
- CelebA dataset in `data/celeba/img_align_celeba/`

Enjoy creating beautiful AI-generated faces! ğŸ­âœ¨
* `genpics.py`: creates a panel of original image + 7 reconstructed ones.
* `vae_model_20.pth`: a trained VAE.

Running a trained model on a CPU is fine. 

Training on a CPU is possible, but slow: âš¡ğŸ‘‰ GPU.
